{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization and abstraction of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    with np.load(\"notMNIST.npz\") as data:\n",
    "        Data, Target = data[\"images\"], data[\"labels\"]\n",
    "        np.random.seed(521)\n",
    "        randIndx = np.arange(len(Data))\n",
    "        np.random.shuffle(randIndx)\n",
    "        Data = Data[randIndx] / 255.0\n",
    "        Target = Target[randIndx]\n",
    "        trainData, trainTarget = Data[:10000], Target[:10000]\n",
    "        validData, validTarget = Data[10000:16000], Target[10000:16000]\n",
    "        testData, testTarget = Data[16000:], Target[16000:]\n",
    "    return trainData, validData, testData, trainTarget, validTarget, testTarget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertOneHot(trainTarget, validTarget, testTarget):\n",
    "    newtrain = np.zeros((trainTarget.shape[0], 10))\n",
    "    newvalid = np.zeros((validTarget.shape[0], 10))\n",
    "    newtest = np.zeros((testTarget.shape[0], 10))\n",
    "\n",
    "    for item in range(0, trainTarget.shape[0]):\n",
    "        newtrain[item][trainTarget[item]] = 1\n",
    "    for item in range(0, validTarget.shape[0]):\n",
    "        newvalid[item][validTarget[item]] = 1\n",
    "    for item in range(0, testTarget.shape[0]):\n",
    "        newtest[item][testTarget[item]] = 1\n",
    "    return newtrain, newvalid, newtest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(trainData, trainTarget):\n",
    "    np.random.seed(421)\n",
    "    randIndx = np.arange(len(trainData))\n",
    "    target = trainTarget\n",
    "    np.random.shuffle(randIndx)\n",
    "    data, target = trainData[randIndx], target[randIndx]\n",
    "    return data, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n"
     ]
    }
   ],
   "source": [
    "trainData, validData, testData, trainTarget, validTarget, testTarget= loadData()\n",
    "train_y, valid_y, test_y = convertOneHot( trainTarget, validTarget, testTarget)\n",
    "#Train Data \n",
    "train_x = trainData.reshape(10000,784)\n",
    "train_X = train_x.T\n",
    "#X0 = np.ones((3500,1))\n",
    "#train_X = np.append(X0,train_X,axis=1)\n",
    "\n",
    "#Test Data\n",
    "global test_X\n",
    "test_X = testData.reshape(2724,784)\n",
    "\n",
    "\n",
    "#Validation Data\n",
    "global validation_X\n",
    "validation_X = validData.reshape(6000,784)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#weights and bias from input to hidden layer ---> W, b\n",
    "mu = 0\n",
    "sigma =1 /  math.sqrt(784 + 1000)\n",
    "W = np.random.normal(mu, sigma, [784,1000])\n",
    "bW = np.zeros((1,1000))\n",
    "\n",
    "\n",
    "#weights and bias from hidden to output layer ---> V,bV\n",
    "mu = 0\n",
    "sigma =1/ math.sqrt(1000 + 10)\n",
    "V = np.random.normal(mu, sigma, [1000,10])\n",
    "bV = np.zeros((1,10))\n",
    "#print(V.shape)\n",
    "#print(bV.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(x) : \n",
    "    return np.maximum(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z) :\n",
    "    b = np.amax(z,axis=0)\n",
    "    e_pow = np.exp(z-b)\n",
    "    summation = np.sum(e_pow,0)\n",
    "    out = np.divide(e_pow,summation)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeLayer(X,W,b) :\n",
    "    return np.dot(W.T,X) + b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### need to check inf * 0 problem\n",
    "def CE(target, prediction):\n",
    "    N = target.shape[0]\n",
    "    #check assignment anouncement\n",
    "    ### bug ###\n",
    "    #prediction[prediction == 0] = 0.0000001\n",
    "    log_term = np.log(prediction)\n",
    "    CE_out = np.multiply(target,log_term)\n",
    "    CE_out = (np.sum(CE_out)) / N\n",
    "    return -1*CE_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradCE(target, prediction):\n",
    "\n",
    "    # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "\n",
    "def forward_pass(train_X,W,bW,V,bV) :\n",
    "    S1 = computeLayer(train_X,W,bW)\n",
    "    #print(\"S1 >> \",S1.shape)\n",
    "    #print(S1)\n",
    "    Y = relu(S1)\n",
    "    #print(Y)\n",
    "    #print(\"Y >> \",Y.shape)\n",
    "    S2 = computeLayer(Y,V,bV)\n",
    "    #print(S2)\n",
    "    #print(\"S2 >> \",S2.shape)\n",
    "    Z = softmax(S2)\n",
    "    #print(Z)\n",
    "    #print(\"Z >> \",Z.shape)\n",
    "\n",
    "    #test = np.sum(Z,0)\n",
    "    #for i in range(10000) :\n",
    "    #    if(test[i] != 1) :\n",
    "    #        print(test[i])\n",
    "\n",
    "    Z_1 = Z.T\n",
    "\n",
    "    loss_out = CE(train_y, Z_1)\n",
    "    return loss_out,S1,Y,S2,Z,Z_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Gradient wrt outer layer weights \n",
    "# dL/dV(i,j) = [ dL/ds2(j) ] * [ ds2(j)/dV(i,j) ]\n",
    "# tot_Wo        = term_1 * term_2\n",
    "# dL/ds2(j) = [ dL/dZ(j) ] * [ dZ(j)/dS2(j) ]\n",
    "# term_1     = term_1_1 * term_1_2\n",
    "def grad_wo(Y,Z_1,train_y,S2):\n",
    "    term_2 = np.copy(Y)\n",
    "\n",
    "    #print(\"term_2 >> \",term_2.shape)\n",
    "    #out = (train_y / Z_1)\n",
    "    term_1_1 = -1*(1/10000)*np.sum((train_y / Z_1),0)\n",
    "    #print(\"term_1_1 >> \",term_1_1)\n",
    "    \n",
    "    b = np.amax(S2,axis=0)\n",
    "    e_pow = np.exp(S2-b)\n",
    "    summation = np.sum(e_pow,0)\n",
    "    term_1_2 =  np.divide(e_pow,summation)\n",
    "    #print(\"term_1_2 >> \",term_1_2.shape)\n",
    "    \n",
    "    ###########################\n",
    "    ###### problem with o1\n",
    "    ###### value large\n",
    "    ###### multiplying by 1/10000 thats wrong\n",
    "    ###### applies for all the gradient calculations\n",
    "    ###########################\n",
    "    o1 = np.matmul(term_1_2,term_2.T)\n",
    "    #print(\"o1 >> \",o1)\n",
    "    term_1_1_1 = term_1_1.reshape(10,1)\n",
    "    #print(\"term_1_1_1 >> \",term_1_1_1)\n",
    "    tot_Wo = np.multiply(o1.T,term_1_1_1.T)\n",
    "    return (1/10000)*tot_Wo\n",
    "    #print(tot_Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#### Gradient wrt outer layer bias \n",
    "# dL/dV(i,j) = [ dL/ds2(j) ] * [ ds2(j)/dVb(i,j) ]\n",
    "# tot_Wo        = term_1 * 1\n",
    "# dL/ds2(j) = [ dL/dZ(j) ] * [ dZ(j)/dS2(j) ]\n",
    "# term_1     = term_1_1 * term_1_2\n",
    "def grad_bo(Z_1,S2,train_y):\n",
    "\n",
    "    term_2 = np.ones((1,10000))\n",
    "\n",
    "\n",
    "    out = (train_y / Z_1)\n",
    "    term_1_1 = -1*(1/10000)*np.sum((train_y / Z_1),0)\n",
    "\n",
    "    b = np.amax(S2,axis=0)\n",
    "    e_pow = np.exp(S2-b)\n",
    "    summation = np.sum(e_pow,0)\n",
    "    term_1_2 =  np.divide(e_pow,summation)\n",
    "\n",
    "\n",
    "    o1 = np.matmul(term_1_2,term_2.T)\n",
    "\n",
    "    term_1_1_1 = term_1_1.reshape(10,1)\n",
    "    tot_bo = np.multiply(o1.T,term_1_1_1.T)\n",
    "    return (1/10000)*tot_bo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Gradient wrt hidden layer weight \n",
    "# dL/dW(i,j) = [ dL/ds1(j) ] * [ ds1(j)/dW(i,j) ]\n",
    "# tot_Wh        = term_1 * term_2\n",
    "def grad_wh(train_X,Y,tot_Wo,V):\n",
    "\n",
    "    term_2 = np.copy(train_X)\n",
    "    # dL/ds1(j) = [ dL/dY(j) ] * [ dY(j)/dS1(j) ]\n",
    "    # term_1     = term_1_1 * term_1_2\n",
    "\n",
    "\n",
    "    # dY(j)/dS1(j) = differentiation of relu \n",
    "    term_1_2 = np.copy(Y)\n",
    "    term_1_2[term_1_2 > 0] = 1\n",
    "    # dL/dY(j) = sumation([dL/dS2] * [dS2/dY])\n",
    "    # term_1_1 = term_1_1_1 * term_1_1_2\n",
    "\n",
    "    term_1_1_1 = np.copy(tot_Wo)\n",
    "    term_1_1_2 = np.copy(V)\n",
    "    ########\n",
    "    ##check (1/1000)\n",
    "    ########\n",
    "    term_1_1 = np.sum(term_1_1_1*term_1_1_2,1)\n",
    "    o1 = np.matmul(term_2,term_1_2.T)\n",
    "    term_1_1_up = term_1_1.reshape(1000,1)\n",
    "    tot_Wh = np.multiply(term_1_1_up.T,o1)\n",
    "    return (1/10000)*tot_Wh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Gradient wrt hidden layer bias \n",
    "# dL/dW(i,j) = [ dL/ds1(j) ] * [ ds1(j)/db(i,j) ]\n",
    "# tot_bh        = term_1 * term_2\n",
    "def grad_bh(Y,tot_Wo,V):\n",
    "\n",
    "    term_2 = np.ones((1,10000))\n",
    "    # dL/ds1(j) = [ dL/dY(j) ] * [ dY(j)/dS1(j) ]\n",
    "    # term_1     = term_1_1 * term_1_2\n",
    "\n",
    "    # dY(j)/dS1(j) = differentiation of relu \n",
    "    term_1_2 = np.copy(Y)\n",
    "    term_1_2[term_1_2 > 0] = 1\n",
    "    # dL/dY(j) = sumation([dL/dS2] * [dS2/dY])\n",
    "    # term_1_1 = term_1_1_1 * term_1_1_2\n",
    "\n",
    "    term_1_1_1 = np.copy(tot_Wo)\n",
    "    term_1_1_2 = np.copy(V)\n",
    "\n",
    "    term_1_1 = np.sum(term_1_1_1*term_1_1_2,1)\n",
    "\n",
    "    o1 = np.matmul(term_2,term_1_2.T)\n",
    "    term_1_1_up = term_1_1.reshape(1000,1)\n",
    "    tot_bh = np.multiply(term_1_1_up.T,o1)\n",
    "    return (1/10000)*tot_bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.337607699149269\n",
      "2.3366547891555034\n"
     ]
    }
   ],
   "source": [
    "mV = np.full((1000,10),0.00001)\n",
    "mW = np.full((784,1000),0.00001)\n",
    "mbV = np.full((1,10),0.00001)\n",
    "mb = np.full((1,1000),0.00001)\n",
    "gamma = 0.9\n",
    "alpha = 0.01\n",
    "\n",
    "\n",
    "mu = 0\n",
    "sigma =1 /  math.sqrt(784 + 1000)\n",
    "W = np.random.normal(mu, sigma, [784,1000])\n",
    "bW = np.zeros((1,1000))\n",
    "\n",
    "\n",
    "#weights and bias from hidden to output layer ---> V,bV\n",
    "mu = 0\n",
    "sigma =1/ math.sqrt(1000 + 10)\n",
    "V = np.random.normal(mu, sigma, [1000,10])\n",
    "bV = np.zeros((1,10))\n",
    "\n",
    "for i in range(2) :\n",
    "    loss_out,S1,Y,S2,Z,Z_1 = forward_pass(train_X,W,bW,V,bV)\n",
    "    print(loss_out)\n",
    "    tot_Wo=grad_wo(Y,Z_1,train_y,S2)\n",
    "    #print(tot_Wo)\n",
    "    #print(\"-------\")\n",
    "    mV = gamma*mV + alpha*tot_Wo\n",
    "    #print(V)\n",
    "    #print(\"-------\")\n",
    "    V = V - mV\n",
    "    #print(V)\n",
    "    tot_bo=grad_bo(Z_1,S2,train_y)\n",
    "    mbV = gamma*mbV + alpha*tot_bo\n",
    "    bV = bV - mbV\n",
    "    \n",
    "    tot_Wh=grad_wh(train_X,Y,tot_Wo,V)\n",
    "    mW = gamma*mW + alpha*tot_Wh\n",
    "    W = W - mW\n",
    "    tot_bh=grad_bh(Y,tot_Wo,V)\n",
    "    mb = gamma*mb + alpha*tot_bh\n",
    "    bW = bW - mb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
